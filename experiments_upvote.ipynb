{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 305
        },
        "id": "ZS71veXiFohW",
        "outputId": "596e3dd8-8265-4f95-e0e4-3ea25398807d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing questions that contain IANAL...\n",
            "Page 1 | Returned Items: 12 | Has More: False\n",
            "Processing answers that contain IANAL...\n",
            "Page 1 | Returned Items: 100 | Has More: True\n",
            "Page 2 | Returned Items: 100 | Has More: True\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-b3b740bda082>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    156\u001b[0m                         \u001b[0mprocessed_answer_ids\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m                 \u001b[0;31m# Pause briefly between processing answers.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 158\u001b[0;31m                 \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    159\u001b[0m         \u001b[0;31m# End processing one page of excerpts\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import requests\n",
        "import csv\n",
        "import time\n",
        "\n",
        "\"\"\"\n",
        "Script used to find entries of stack exchange sites that contain the acronym \"IANAL\" and\n",
        "save them to a CSV file. The script will search for questions and answers containing \"IANAL\"\n",
        "before saving the data.\n",
        "\"\"\"\n",
        "\n",
        "# Configuration\n",
        "SITE = 'softwareengineering'  # Change to another Stack Exchange site as needed\n",
        "OUTPUT_FILE = 'ianal_posts.csv'\n",
        "\n",
        "# API endpoints\n",
        "QUESTION_SEARCH_URL = \"https://api.stackexchange.com/2.3/search/advanced\"\n",
        "QUESTION_ANSWERS_URL = \"https://api.stackexchange.com/2.3/questions/{id}/answers\"\n",
        "ANSWER_DETAIL_URL = \"https://api.stackexchange.com/2.3/answers/{id}\"\n",
        "QUESTION_DETAIL_URL = \"https://api.stackexchange.com/2.3/questions/{id}\"\n",
        "EXCERPT_SEARCH_URL = \"https://api.stackexchange.com/2.3/search/excerpts\"\n",
        "\n",
        "# We'll track answer IDs that we have already processed so we don’t duplicate them.\n",
        "processed_answer_ids = set()\n",
        "\n",
        "# Open the CSV file for writing.\n",
        "with open(OUTPUT_FILE, mode='w', newline='', encoding='utf-8') as f:\n",
        "    writer = csv.writer(f)\n",
        "    # CSV header row\n",
        "    writer.writerow([\"post_type\", \"post_id\", \"creation_date\", \"question_id\", \"question_title\", \"question_body\", \"answer_body\"])\n",
        "\n",
        "    ###########################################################################\n",
        "    # PART 1: Process questions that contain IANAL using /search/advanced\n",
        "    ###########################################################################\n",
        "    print(\"Processing questions that contain IANAL...\")\n",
        "    page = 1\n",
        "    while True:\n",
        "        params = {\n",
        "            'q': 'IANAL',       # Search term\n",
        "            'site': SITE,\n",
        "            'pagesize': 100,\n",
        "            'page': page,\n",
        "            'filter': 'withbody'  # Built-in filter to include the full body\n",
        "        }\n",
        "        response = requests.get(QUESTION_SEARCH_URL, params=params)\n",
        "        data = response.json()\n",
        "        if 'items' not in data:\n",
        "            print(\"No more question items or an error occurred.\")\n",
        "            break\n",
        "\n",
        "        items = data.get('items', [])\n",
        "        print(f\"Page {page} | Returned Items: {len(items)} | Has More: {data.get('has_more')}\")\n",
        "\n",
        "        for question in data['items']:\n",
        "            # Double-check that the question body contains \"IANAL\" (case-insensitive)\n",
        "            if \"ianal\" in question.get('body', '').lower():\n",
        "                question_id = question.get('question_id')\n",
        "                creation_date = question.get('creation_date')\n",
        "                title = question.get('title')\n",
        "                question_body = question.get('body')\n",
        "                # Write a row for the question itself\n",
        "                writer.writerow([\"question\", question_id, creation_date, question_id, title, question_body, \"\"])\n",
        "\n",
        "                # Retrieve the answers for this question.\n",
        "                ans_params = {\n",
        "                    'site': SITE,\n",
        "                    'pagesize': 100,\n",
        "                    'filter': 'withbody'\n",
        "                }\n",
        "                ans_url = QUESTION_ANSWERS_URL.format(id=question_id)\n",
        "                ans_response = requests.get(ans_url, params=ans_params)\n",
        "                ans_data = ans_response.json()\n",
        "\n",
        "                # For each answer, check if it also contains \"IANAL\"\n",
        "                if 'items' in ans_data:\n",
        "                    for answer in ans_data['items']:\n",
        "                        answer_body = answer.get('body', '')\n",
        "                        if \"ianal\" in answer_body.lower():\n",
        "                            answer_id = answer.get('answer_id')\n",
        "                            creation_date_ans = answer.get('creation_date')\n",
        "                            # Only add if not already processed\n",
        "                            if answer_id not in processed_answer_ids:\n",
        "                                writer.writerow([\"answer\", answer_id, creation_date_ans, question_id, title, question_body, answer_body])\n",
        "                                processed_answer_ids.add(answer_id)\n",
        "                        # End if answer qualifies\n",
        "                # Pause briefly between question and its answers\n",
        "                time.sleep(0.2)\n",
        "        # End processing of one page of questions\n",
        "\n",
        "        if not data.get('has_more'):\n",
        "            break\n",
        "\n",
        "        page += 1\n",
        "        time.sleep(0.5)\n",
        "\n",
        "    ##################################################################################\n",
        "    # PART 2: Process answers that contain IANAL (but whose parent question did not)\n",
        "    #         using /search/excerpts to catch answers that weren't returned in part 1.\n",
        "    ##################################################################################\n",
        "    print(\"Processing answers that contain IANAL...\")\n",
        "    page = 1\n",
        "    while True:\n",
        "        params = {\n",
        "            'q': 'IANAL',\n",
        "            'site': SITE,\n",
        "            'pagesize': 100,\n",
        "            'page': page\n",
        "        }\n",
        "        response = requests.get(EXCERPT_SEARCH_URL, params=params)\n",
        "        data = response.json()\n",
        "        if 'items' not in data:\n",
        "            print(\"No more excerpt items or an error occurred.\")\n",
        "            break\n",
        "\n",
        "        items = data.get('items', [])\n",
        "        print(f\"Page {page} | Returned Items: {len(items)} | Has More: {data.get('has_more')}\")\n",
        "\n",
        "        for item in data['items']:\n",
        "            # We’re only interested in answers from the excerpt search.\n",
        "            if item.get('item_type') == 'answer':\n",
        "                answer_id = item.get('answer_id')  # In excerpt results, \"post_id\" is the answer id.\n",
        "                # Skip if we've already processed this answer.\n",
        "                if answer_id in processed_answer_ids or answer_id is None:\n",
        "                    continue\n",
        "\n",
        "                # Fetch the full answer details to get the complete body.\n",
        "                ans_detail_url = ANSWER_DETAIL_URL.format(id=answer_id)\n",
        "                ans_params = {\n",
        "                    'site': SITE,\n",
        "                    'filter': 'withbody'\n",
        "                }\n",
        "                ans_detail_response = requests.get(ans_detail_url, params=ans_params)\n",
        "                ans_detail_data = ans_detail_response.json()\n",
        "                if 'items' in ans_detail_data and len(ans_detail_data['items']) > 0:\n",
        "                    answer_detail = ans_detail_data['items'][0]\n",
        "                    answer_body = answer_detail.get('body', '')\n",
        "                    # Ensure the answer body indeed contains IANAL.\n",
        "                    if \"ianal\" not in answer_body.lower():\n",
        "                        continue\n",
        "                    creation_date_ans = answer_detail.get('creation_date')\n",
        "                    question_id = answer_detail.get('question_id')\n",
        "\n",
        "                    # Now fetch the parent question details for context.\n",
        "                    quest_detail_url = QUESTION_DETAIL_URL.format(id=question_id)\n",
        "                    quest_params = {\n",
        "                        'site': SITE,\n",
        "                        'filter': 'withbody'\n",
        "                    }\n",
        "                    quest_detail_response = requests.get(quest_detail_url, params=quest_params)\n",
        "                    quest_detail_data = quest_detail_response.json()\n",
        "                    if 'items' in quest_detail_data and len(quest_detail_data['items']) > 0:\n",
        "                        question_detail = quest_detail_data['items'][0]\n",
        "                        title = question_detail.get('title')\n",
        "                        question_body = question_detail.get('body')\n",
        "                        # Write a row for this answer along with its question context.\n",
        "                        writer.writerow([\"answer\", answer_id, creation_date_ans, question_id, title, question_body, answer_body])\n",
        "                        processed_answer_ids.add(answer_id)\n",
        "                # Pause briefly between processing answers.\n",
        "                time.sleep(0.2)\n",
        "        # End processing one page of excerpts\n",
        "\n",
        "        if not data.get('has_more'):\n",
        "            break\n",
        "\n",
        "        page += 1\n",
        "        time.sleep(0.5)\n",
        "\n",
        "print(\"Data saved to\", OUTPUT_FILE)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import csv\n",
        "import time\n",
        "\n",
        "\"\"\"\n",
        "Script used to find entries of Stack Exchange sites that contain the acronym \"IANAL\" and\n",
        "save them to a CSV file. The script searches for questions and answers containing \"IANAL\"\n",
        "before saving the data.\n",
        "\"\"\"\n",
        "\n",
        "# Configuration\n",
        "SITE = 'softwareengineering'  # Change to another Stack Exchange site as needed\n",
        "OUTPUT_FILE = 'ianal_posts.csv'\n",
        "API_KEY = \"********\"  # Insert your Stack Exchange API key here\n",
        "\n",
        "# API endpoints\n",
        "QUESTION_SEARCH_URL = \"https://api.stackexchange.com/2.3/search/advanced\"\n",
        "QUESTION_ANSWERS_URL = \"https://api.stackexchange.com/2.3/questions/{id}/answers\"\n",
        "ANSWER_DETAIL_URL = \"https://api.stackexchange.com/2.3/answers/{id}\"\n",
        "QUESTION_DETAIL_URL = \"https://api.stackexchange.com/2.3/questions/{id}\"\n",
        "EXCERPT_SEARCH_URL = \"https://api.stackexchange.com/2.3/search/excerpts\"\n",
        "\n",
        "# Helper function to add the API key if it is provided\n",
        "def add_api_key(params):\n",
        "    if API_KEY:\n",
        "        params['key'] = API_KEY\n",
        "    return params\n",
        "\n",
        "# We'll track answer IDs that we have already processed so we don’t duplicate them.\n",
        "processed_answer_ids = set()\n",
        "\n",
        "# Open the CSV file for writing.\n",
        "with open(OUTPUT_FILE, mode='w', newline='', encoding='utf-8') as f:\n",
        "    writer = csv.writer(f)\n",
        "    # CSV header row\n",
        "    writer.writerow([\"post_type\", \"post_id\", \"creation_date\", \"question_id\", \"question_title\", \"question_body\", \"answer_body\"])\n",
        "\n",
        "    ###########################################################################\n",
        "    # PART 1: Process questions that contain IANAL using /search/advanced\n",
        "    ###########################################################################\n",
        "    print(\"Processing questions that contain IANAL...\")\n",
        "    page = 1\n",
        "    while True:\n",
        "        params = {\n",
        "            'q': 'IANAL',       # Search term\n",
        "            'site': SITE,\n",
        "            'pagesize': 100,\n",
        "            'page': page,\n",
        "            'filter': 'withbody'  # Built-in filter to include the full body\n",
        "        }\n",
        "        params = add_api_key(params)  # Add the API key to parameters\n",
        "        response = requests.get(QUESTION_SEARCH_URL, params=params)\n",
        "        data = response.json()\n",
        "        if 'items' not in data:\n",
        "            print(\"No more question items or an error occurred.\")\n",
        "            break\n",
        "\n",
        "        items = data.get('items', [])\n",
        "        print(f\"Page {page} | Returned Items: {len(items)} | Has More: {data.get('has_more')}\")\n",
        "\n",
        "        for question in data['items']:\n",
        "            # Double-check that the question body contains \"IANAL\" (case-insensitive)\n",
        "            if \"ianal\" in question.get('body', '').lower():\n",
        "                question_id = question.get('question_id')\n",
        "                creation_date = question.get('creation_date')\n",
        "                title = question.get('title')\n",
        "                question_body = question.get('body')\n",
        "                # Write a row for the question itself\n",
        "                writer.writerow([\"question\", question_id, creation_date, question_id, title, question_body, \"\"])\n",
        "\n",
        "                # Retrieve the answers for this question.\n",
        "                ans_params = {\n",
        "                    'site': SITE,\n",
        "                    'pagesize': 100,\n",
        "                    'filter': 'withbody'\n",
        "                }\n",
        "                ans_params = add_api_key(ans_params)\n",
        "                ans_url = QUESTION_ANSWERS_URL.format(id=question_id)\n",
        "                ans_response = requests.get(ans_url, params=ans_params)\n",
        "                ans_data = ans_response.json()\n",
        "\n",
        "                # For each answer, check if it also contains \"IANAL\"\n",
        "                if 'items' in ans_data:\n",
        "                    for answer in ans_data['items']:\n",
        "                        answer_body = answer.get('body', '')\n",
        "                        if \"ianal\" in answer_body.lower():\n",
        "                            answer_id = answer.get('answer_id')\n",
        "                            creation_date_ans = answer.get('creation_date')\n",
        "                            # Only add if not already processed\n",
        "                            if answer_id not in processed_answer_ids:\n",
        "                                writer.writerow([\"answer\", answer_id, creation_date_ans, question_id, title, question_body, answer_body])\n",
        "                                processed_answer_ids.add(answer_id)\n",
        "                # Pause briefly between question and its answers\n",
        "                time.sleep(0.2)\n",
        "        # End processing of one page of questions\n",
        "\n",
        "        if not data.get('has_more'):\n",
        "            break\n",
        "\n",
        "        page += 1\n",
        "        time.sleep(0.5)\n",
        "\n",
        "    ##################################################################################\n",
        "    # PART 2: Process answers that contain IANAL (but whose parent question did not)\n",
        "    #         using /search/excerpts to catch answers that weren't returned in part 1.\n",
        "    ##################################################################################\n",
        "    print(\"Processing answers that contain IANAL...\")\n",
        "    page = 1\n",
        "    while True:\n",
        "        params = {\n",
        "            'q': 'IANAL',\n",
        "            'site': SITE,\n",
        "            'pagesize': 100,\n",
        "            'page': page\n",
        "        }\n",
        "        params = add_api_key(params)\n",
        "        response = requests.get(EXCERPT_SEARCH_URL, params=params)\n",
        "        data = response.json()\n",
        "        if 'items' not in data:\n",
        "            print(\"No more excerpt items or an error occurred.\")\n",
        "            break\n",
        "\n",
        "        items = data.get('items', [])\n",
        "        print(f\"Page {page} | Returned Items: {len(items)} | Has More: {data.get('has_more')}\")\n",
        "\n",
        "        for item in data['items']:\n",
        "            # We’re only interested in answers from the excerpt search.\n",
        "            if item.get('item_type') == 'answer':\n",
        "                answer_id = item.get('answer_id')  # In excerpt results, \"post_id\" is the answer id.\n",
        "                # Skip if we've already processed this answer.\n",
        "                if answer_id in processed_answer_ids or answer_id is None:\n",
        "                    continue\n",
        "\n",
        "                # Fetch the full answer details to get the complete body.\n",
        "                ans_detail_url = ANSWER_DETAIL_URL.format(id=answer_id)\n",
        "                ans_params = {\n",
        "                    'site': SITE,\n",
        "                    'filter': 'withbody'\n",
        "                }\n",
        "                ans_params = add_api_key(ans_params)\n",
        "                ans_detail_response = requests.get(ans_detail_url, params=ans_params)\n",
        "                ans_detail_data = ans_detail_response.json()\n",
        "                if 'items' in ans_detail_data and len(ans_detail_data['items']) > 0:\n",
        "                    answer_detail = ans_detail_data['items'][0]\n",
        "                    answer_body = answer_detail.get('body', '')\n",
        "                    # Ensure the answer body indeed contains IANAL.\n",
        "                    if \"ianal\" not in answer_body.lower():\n",
        "                        continue\n",
        "                    creation_date_ans = answer_detail.get('creation_date')\n",
        "                    question_id = answer_detail.get('question_id')\n",
        "\n",
        "                    # Now fetch the parent question details for context.\n",
        "                    quest_detail_url = QUESTION_DETAIL_URL.format(id=question_id)\n",
        "                    quest_params = {\n",
        "                        'site': SITE,\n",
        "                        'filter': 'withbody'\n",
        "                    }\n",
        "                    quest_params = add_api_key(quest_params)\n",
        "                    quest_detail_response = requests.get(quest_detail_url, params=quest_params)\n",
        "                    quest_detail_data = quest_detail_response.json()\n",
        "                    if 'items' in quest_detail_data and len(quest_detail_data['items']) > 0:\n",
        "                        question_detail = quest_detail_data['items'][0]\n",
        "                        title = question_detail.get('title')\n",
        "                        question_body = question_detail.get('body')\n",
        "                        # Write a row for this answer along with its question context.\n",
        "                        writer.writerow([\"answer\", answer_id, creation_date_ans, question_id, title, question_body, answer_body])\n",
        "                        processed_answer_ids.add(answer_id)\n",
        "                # Pause briefly between processing answers.\n",
        "                time.sleep(0.2)\n",
        "        # End processing one page of excerpts\n",
        "\n",
        "        if not data.get('has_more'):\n",
        "            break\n",
        "\n",
        "        page += 1\n",
        "        time.sleep(0.5)\n",
        "\n",
        "print(\"Data saved to\", OUTPUT_FILE)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "72LxdhMSGG_U",
        "outputId": "0f52dfef-a4b9-4cc4-9c9b-479b5597b626"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing questions that contain IANAL...\n",
            "Page 1 | Returned Items: 12 | Has More: False\n",
            "Processing answers that contain IANAL...\n",
            "Page 1 | Returned Items: 100 | Has More: True\n",
            "Page 2 | Returned Items: 100 | Has More: True\n",
            "Page 3 | Returned Items: 76 | Has More: False\n",
            "Data saved to ianal_posts.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import csv\n",
        "import time\n",
        "\n",
        "\"\"\"\n",
        "Script used to find entries of Stack Exchange sites that contain the acronym \"IANAL\" and\n",
        "retrieve all answers for those questions, along with upvotes and number of answers per question.\n",
        "\"\"\"\n",
        "\n",
        "# Configuration\n",
        "SITE = 'devops'  # Change to another Stack Exchange site as needed\n",
        "OUTPUT_FILE = 'devops.csv'\n",
        "API_KEY = '********'  # Add your API key here for higher rate limits (optional)\n",
        "\n",
        "# API endpoints\n",
        "ANSWER_SEARCH_URL = \"https://api.stackexchange.com/2.3/search/excerpts\"\n",
        "QUESTION_ANSWERS_URL = \"https://api.stackexchange.com/2.3/questions/{id}/answers\"\n",
        "QUESTION_DETAIL_URL = \"https://api.stackexchange.com/2.3/questions/{id}\"\n",
        "\n",
        "def make_request(url, params):\n",
        "    \"\"\"Helper function to make API requests with retries and error handling.\"\"\"\n",
        "    for _ in range(3):  # Retry up to 3 times in case of failure\n",
        "        try:\n",
        "            response = requests.get(url, params=params)\n",
        "            if response.status_code == 200:\n",
        "                return response.json()\n",
        "            elif response.status_code == 429:\n",
        "                print(\"Rate limit exceeded. Waiting before retrying...\")\n",
        "                time.sleep(5)\n",
        "            else:\n",
        "                print(f\"Error: Received status code {response.status_code}\")\n",
        "                return None\n",
        "        except requests.exceptions.RequestException as e:\n",
        "            print(f\"Request failed: {e}\")\n",
        "            time.sleep(2)\n",
        "    return None\n",
        "\n",
        "with open(OUTPUT_FILE, mode='w', newline='', encoding='utf-8') as f:\n",
        "    writer = csv.writer(f)\n",
        "    writer.writerow([\"question_id\", \"question_title\", \"question_body\", \"num_answers\", \"answers_with_upvotes\", \"majority\"])\n",
        "\n",
        "    print(\"Searching for answers that explicitly contain 'IANAL'...\")\n",
        "    page = 1\n",
        "    while True:\n",
        "        params = {\n",
        "            'q': 'IANAL',\n",
        "            'site': SITE,\n",
        "            'pagesize': 100,\n",
        "            'page': page,\n",
        "            'filter': 'withbody',\n",
        "            'item_type': 'answer',\n",
        "            'key': API_KEY\n",
        "        }\n",
        "        data = make_request(ANSWER_SEARCH_URL, params)\n",
        "        if not data or 'items' not in data:\n",
        "            break\n",
        "\n",
        "        question_ids = set()\n",
        "        for item in data.get('items', []):\n",
        "            if item.get('item_type') == 'answer':\n",
        "                question_ids.add(item.get('question_id'))\n",
        "\n",
        "        for question_id in question_ids:\n",
        "            # Fetch question details\n",
        "            quest_params = {'site': SITE, 'filter': 'withbody', 'key': API_KEY}\n",
        "            quest_data = make_request(QUESTION_DETAIL_URL.format(id=question_id), quest_params)\n",
        "\n",
        "            # Fetch all answers for the question\n",
        "            ans_params = {'site': SITE, 'pagesize': 100, 'filter': 'withbody', 'key': API_KEY}\n",
        "            ans_data = make_request(QUESTION_ANSWERS_URL.format(id=question_id), ans_params)\n",
        "\n",
        "            if not quest_data or 'items' not in quest_data or not ans_data or 'items' not in ans_data:\n",
        "                continue\n",
        "\n",
        "            question_detail = quest_data['items'][0]\n",
        "            question_title = question_detail.get('title', '')\n",
        "            question_body = question_detail.get('body', '')\n",
        "            num_answers = len(ans_data['items'])\n",
        "\n",
        "            # Store all answers and their upvotes in a structured format\n",
        "            answers_with_upvotes = []\n",
        "            max_upvotes = 0\n",
        "            ianal_answer_max = False\n",
        "            ianal_present = False\n",
        "\n",
        "            for answer in ans_data['items']:\n",
        "                answer_id = answer.get('answer_id')\n",
        "                answer_body = answer.get('body', '').replace('\\n', ' ').replace('\\r', ' ')\n",
        "                upvotes = answer.get('score', 0)\n",
        "                answers_with_upvotes.append(f\"[Answer ID: {answer_id}, Upvotes: {upvotes}] {answer_body}\")\n",
        "\n",
        "                # Determine max upvoted answer\n",
        "                if upvotes > max_upvotes:\n",
        "                    max_upvotes = upvotes\n",
        "                    ianal_answer_max = \"ianal\" in answer_body.lower()\n",
        "\n",
        "                if \"ianal\" in answer_body.lower():\n",
        "                    ianal_present = True\n",
        "\n",
        "            # Determine majority field\n",
        "            if num_answers == 1:\n",
        "                majority = \"NA\"\n",
        "            else:\n",
        "                majority = \"True\" if ianal_present and ianal_answer_max else \"False\"\n",
        "\n",
        "            writer.writerow([question_id, question_title, question_body, num_answers, \" | \".join(answers_with_upvotes), majority])\n",
        "\n",
        "            time.sleep(0.2)\n",
        "\n",
        "        if not data.get('has_more'):\n",
        "            break\n",
        "\n",
        "        page += 1\n",
        "        time.sleep(1)\n",
        "\n",
        "print(f\"Data saved to {OUTPUT_FILE}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B5ohAnc1Gaqt",
        "outputId": "fd43782f-b924-486a-920f-351623e6531c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Searching for answers that explicitly contain 'IANAL'...\n",
            "Data saved to devops.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "def compile_csv_files(directory_path, output_file=\"uvcompiled.csv\"):\n",
        "    # List all CSV files in the provided directory\n",
        "    csv_files = [file for file in os.listdir(directory_path) if file.endswith(\".csv\")]\n",
        "\n",
        "    # Create an empty list to store DataFrames\n",
        "    dataframes = []\n",
        "\n",
        "    # Loop over all CSV files and read them into pandas\n",
        "    for csv_file in csv_files:\n",
        "        file_path = os.path.join(directory_path, csv_file)\n",
        "        df = pd.read_csv(file_path)\n",
        "        dataframes.append(df)\n",
        "\n",
        "    # Concatenate all DataFrames into a single DataFrame\n",
        "    combined_df = pd.concat(dataframes, ignore_index=True)\n",
        "\n",
        "    # Save the combined DataFrame to a new CSV file\n",
        "    combined_df.to_csv(output_file, index=False)\n",
        "    print(f\"Compiled {len(csv_files)} CSV files into '{output_file}'.\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Replace 'path_to_your_csv_folder' with your actual folder path\n",
        "    directory = \"/content/uv\"\n",
        "    compile_csv_files(directory, \"uvcompiled.csv\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NAnzBZc4hFyk",
        "outputId": "1c596e44-f2bf-4051-e222-d3a8c844f4d2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Compiled 11 CSV files into 'uvcompiled.csv'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the dataset (Replace with your actual file path if running locally)\n",
        "file_path = \"uvcompiled.csv\"\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "# Check if the column exists\n",
        "if \"majority\" in df.columns:  # Adjust column name if needed\n",
        "    # Count occurrences of True, False, and NA values\n",
        "    true_count = (df[\"majority\"] == True).sum()\n",
        "    false_count = (df[\"majority\"] == False).sum()\n",
        "    na_count = df[\"majority\"].isna().sum()  # If NA values exist\n",
        "\n",
        "    # Print the results\n",
        "    print(f\"True Count: {true_count}\")\n",
        "    print(f\"False Count: {false_count}\")\n",
        "    print(f\"NA Count: {na_count}\")\n",
        "\n",
        "else:\n",
        "    print(\"Column 'upvote_comparison' not found in dataset. Please check the column names.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uexu5kSthaZd",
        "outputId": "1d62c0e5-6c33-494e-f297-a558e9371baf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True Count: 128\n",
            "False Count: 214\n",
            "NA Count: 180\n"
          ]
        }
      ]
    }
  ]
}