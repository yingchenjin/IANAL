# -*- coding: utf-8 -*-
"""simplify.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1zv5gW-JH2u4YMglH3Nmbx5P8CBf7fKgy
"""

import pandas as pd

# 1. Load your original CSV
df = pd.read_csv("data-ianal.csv")

# 2. Filter out rows where post_type is "question"
df_answers_only = df[df["post_type"] == "answer"].copy()

# 3. Write the resulting DataFrame to a new CSV
#    This CSV will keep the original columns but contain only answer rows.
df_answers_only.to_csv("answers_only.csv", index=False)

print("Filtered CSV created with only 'answer' rows!")

!pip install transformers torch

import pandas as pd
from transformers import pipeline

df = pd.read_csv("answers_only.csv")

!nvidia-smi

#summarizer = pipeline("summarization", model="facebook/bart-large-cnn")
summarizer = pipeline("summarization", model="sshleifer/distilbart-cnn-12-6", device=0)

# Commented out IPython magic to ensure Python compatibility.
# %env CUDA_LAUNCH_BLOCKING=1

def clean_text(text):
    if not isinstance(text, str):
        return ""
    # Remove non-printable / control chars:
    text = "".join(ch for ch in text if ch.isprintable())
    return text

df["question_body"] = df["question_body"].apply(clean_text)
df["answer_body"]   = df["answer_body"].apply(clean_text)

summarizer = pipeline(
    "summarization",
    model="facebook/bart-large-cnn",
    device=-1  # forces CPU usage
)

def summarize_text(text, max_len=60):
    """
    Summarize the given text using the summarizer pipeline.
    Adjust the max_length and min_length as needed.
    """
    # You can tweak these hyperparameters
    result = summarizer(text, max_length=max_len, min_length=10, do_sample=False)
    return result[0]["summary_text"]

# Apply the summarization to the 'body' column

df["question_summary"] = df["question_body"].apply(summarize_text)

# Summarize the answer text
df["answer_summary"] = df["answer_body"].apply(summarize_text)

!pip install --upgrade transformers datasets torch

!pip install torchvision

import pandas as pd
from datasets import Dataset
from transformers import pipeline
import torch

summarizer = pipeline(
    "summarization",
    model="sshleifer/distilbart-cnn-12-6",
    device=0  # 0 for GPU; -1 for CPU
)

# 1. Load the CSV
df = pd.read_csv("data-ianal.csv")

# (Optional) Check the first few rows/columns
print(df.head())
print(df.columns)

def chunk_text(text, chunk_size=512):
    """
    Splits a string into a list of smaller strings of up to `chunk_size` characters.
    """
    if not isinstance(text, str) or not text.strip():
        return []
    return [text[i:i+chunk_size] for i in range(0, len(text), chunk_size)]

def batch_chunk_summarize(batch, chunk_size=512, max_len=60):
    """
    Summarizes both 'question_body' and 'answer_body' in a single pass.
    Assumes 'batch' is a dict with lists of question_body and answer_body.
    Returns new lists: question_summary, answer_summary.
    """
    q_summaries = []
    a_summaries = []

    for qtext, atext in zip(batch["question_body"], batch["answer_body"]):

        # 1) Chunk the question
        q_chunks = chunk_text(qtext, chunk_size=chunk_size)
        q_partials = []
        for chunk in q_chunks:
            out = summarizer(chunk, max_length=max_len, min_length=10, do_sample=False)
            q_partials.append(out[0]["summary_text"])

        # Re-summarize partial Q summaries if there's more than one
        if len(q_partials) > 1:
            combined_q = " ".join(q_partials)
            out = summarizer(combined_q, max_length=max_len, min_length=10, do_sample=False)
            final_q = out[0]["summary_text"]
        elif q_partials:
            final_q = q_partials[0]
        else:
            final_q = ""

        # 2) Chunk the answer
        a_chunks = chunk_text(atext, chunk_size=chunk_size)
        a_partials = []
        for chunk in a_chunks:
            out = summarizer(chunk, max_length=max_len, min_length=10, do_sample=False)
            a_partials.append(out[0]["summary_text"])

        # Re-summarize partial A summaries if there's more than one
        if len(a_partials) > 1:
            combined_a = " ".join(a_partials)
            out = summarizer(combined_a, max_length=max_len, min_length=10, do_sample=False)
            final_a = out[0]["summary_text"]
        elif a_partials:
            final_a = a_partials[0]
        else:
            final_a = ""

        # Add results to our lists
        q_summaries.append(final_q)
        a_summaries.append(final_a)

    return {"question_summary": q_summaries, "answer_summary": a_summaries}

dataset = Dataset.from_pandas(df)  # automatically detects columns
dataset

summarized_dataset = dataset.map(
    batch_chunk_summarize,
    batched=True,
    batch_size=4,  # adjust for your GPU memory
    fn_kwargs={"chunk_size": 512, "max_len": 60}
)

print(summarized_dataset[0])
# e.g., {'question_body': "...", 'answer_body': "...", 'question_summary': "...", 'answer_summary': "...", ...}

df_summarized = summarized_dataset.to_pandas()
df_summarized[["question_body", "answer_body", "question_summary", "answer_summary"]].head(5)

df_summarized.to_csv("data-ianal_summarized.csv", index=False)

!pip install PyPDF2

import PyPDF2

license_texts = {}

pdf_files = [
    "Apache License, Version 2.0 – Open Source Initiative.pdf",
    "Apache Software License, version 1.1 – Open Source Initiative.pdf",
    "GNU Affero General Public License version 3 – Open Source Initiative.pdf",
    "GNU General Public License version 2 – Open Source Initiative.pdf",
    "GNU General Public License version 3 – Open Source Initiative.pdf",
    "GNU Lesser General Public License version 2.1 – Open Source Initiative.pdf",
    "GNU Lesser General Public License version 3 – Open Source Initiative.pdf",
    "GNU Library General Public License version 2 – Open Source Initiative.pdf",
    "The MIT License – Open Source Initiative.pdf",
    "The Nethack General Public License – Open Source Initiative.pdf"
]

for pdf_file in pdf_files:
    with open(f"{pdf_file}", "rb") as f:
        reader = PyPDF2.PdfReader(f)
        text_content = []
        for page in reader.pages:
            page_text = page.extract_text()
            if page_text:
                text_content.append(page_text)
        # Join all pages into a single string
        joined_text = "\n".join(text_content)
        # Store it in a dictionary keyed by filename or short name
        license_texts[pdf_file] = joined_text

# Now license_texts is a dict: { filename: full_license_string, ... }
print(license_texts.keys())

# Suppose you already have `license_texts` like this:
# license_texts = {
#     "Apache License, Version 2.0 – Open Source Initiative.pdf": "...",
#     "Apache Software License, version 1.1 – Open Source Initiative.pdf": "...",
#     "GNU Affero General Public License version 3 – Open Source Initiative.pdf": "...",
#     ...
#     "The MIT License – Open Source Initiative.pdf": "...",
#     "The Nethack General Public License – Open Source Initiative.pdf": "..."
# }

# 1. Define a mapping from long filenames to short license keys:
mapping = {
    "Apache License, Version 2.0 – Open Source Initiative.pdf": "Apache-2.0",
    "Apache Software License, version 1.1 – Open Source Initiative.pdf": "Apache-1.1",
    "GNU Affero General Public License version 3 – Open Source Initiative.pdf": "AGPL-3.0",
    "GNU General Public License version 2 – Open Source Initiative.pdf": "GPL-2.0",
    "GNU General Public License version 3 – Open Source Initiative.pdf": "GPL-3.0",
    "GNU Lesser General Public License version 2.1 – Open Source Initiative.pdf": "LGPL-2.1",
    "GNU Lesser General Public License version 3 – Open Source Initiative.pdf": "LGPL-3.0",
    "GNU Library General Public License version 2 – Open Source Initiative.pdf": "LGPL-2.0",
    "The MIT License – Open Source Initiative.pdf": "MIT",
    "The Nethack General Public License – Open Source Initiative.pdf": "Nethack"
}

# 2. Create a new dictionary keyed by the short license names:
license_map = {}
for long_filename, short_key in mapping.items():
    # Access the text from the old dictionary, license_texts
    license_map[short_key] = license_texts[long_filename]

# 3. Now `license_map` is keyed by short license names instead of the big PDF filenames.
print(license_map.keys())
# e.g. dict_keys(['Apache-2.0', 'Apache-1.1', 'AGPL-3.0', 'GPL-2.0', 'GPL-3.0', 'LGPL-2.1', 'LGPL-3.0', 'LGPL-2.0', 'MIT', 'Nethack'])

# 4. You can now reference, for instance, the full Apache 2.0 text with:
apache2_text = license_map["Apache-2.0"]
print(apache2_text[:500])  # Print first 500 characters, for example

df_summarized["answer_summary"]

from sentence_transformers import SentenceTransformer, util

# 1. Load (or reuse) a sentence-transformer model
model = SentenceTransformer("all-MiniLM-L6-v2")

def compare_to_license(text, license_text):
    """
    Returns a similarity score between the text and the license text.
    Higher score => more semantic overlap.
    """
    if not isinstance(text, str) or not text.strip():
        # If the text is empty or not a string, similarity is 0
        return 0.0

    # 2. Encode both the text and the license text
    text_emb = model.encode(text, convert_to_tensor=True)
    license_emb = model.encode(license_text, convert_to_tensor=True)

    # 3. Compute the cosine similarity
    similarity = util.cos_sim(text_emb, license_emb)

    # 'similarity' is a 2D tensor of shape [1,1], so extract float
    return float(similarity[0][0])

mit_text = license_map["MIT"]  # The string you extracted

# Compare with your existing similarity or LLM-based approach
similarity = compare_to_license(df_summarized["answer_summary"][0], mit_text)

df_summarized["similarity_mit"] = df_summarized["answer_summary"].apply(
    lambda txt: compare_to_license(txt, mit_text)
)

licenses = ["MIT", "Apache-2.0", "GPL-3.0"]  # your short keys
for lic in licenses:
    license_text = license_map[lic]
    col_name = f"similarity_{lic}"
    df_summarized[col_name] = df_summarized["answer_summary"].apply(
        lambda txt: compare_to_license(txt, license_text)
    )

import pandas as pd
from sentence_transformers import SentenceTransformer, util

# Suppose you have:
# 1) df_summarized (DataFrame) with a column "answer_summary"
# 2) a license_map dict of short license name -> license text
#    e.g. license_map = {"MIT": "MIT license text ...", "Apache-2.0": "...", etc.}

# Step 1: Load or reuse your model (once)
model = SentenceTransformer("all-MiniLM-L6-v2")

def compare_to_license(text, license_text):
    """Return a float similarity between text and license_text."""
    if not isinstance(text, str) or not text.strip():
        return 0.0
    txt_emb = model.encode(text, convert_to_tensor=True)
    lic_emb = model.encode(license_text, convert_to_tensor=True)
    similarity = util.cos_sim(txt_emb, lic_emb)
    return float(similarity[0][0])

# Let's assume you have some set of license keys in your license_map:
licenses = ["MIT", "Apache-2.0", "GPL-2.0", "GPL-3.0"]  # example

# Step 2: For each license, compute similarity for each row
for lic in licenses:
    license_text = license_map[lic]
    col_name = f"similarity_{lic}"
    df_summarized[col_name] = df_summarized["answer_summary"].apply(
        lambda x: compare_to_license(x, license_text)
    )

# Step 3: Interpretation / Thresholding
# Example thresholds:
HIGH_THRESHOLD = 0.7
LOW_THRESHOLD  = 0.2

# We can create flags for each row, e.g. "high similarity" or "low similarity"
df_summarized["license_max_sim"] = df_summarized[[f"similarity_{l}" for l in licenses]].max(axis=1)
df_summarized["license_min_sim"] = df_summarized[[f"similarity_{l}" for l in licenses]].min(axis=1)

# We'll mark as "high" if the maximum sim is above 0.7
df_summarized["high_sim_flag"] = df_summarized["license_max_sim"] > HIGH_THRESHOLD
df_summarized["low_sim_flag"]  = df_summarized["license_max_sim"] < LOW_THRESHOLD

# Step 4: Combine with other checks
# For instance, a placeholder function to do a contradiction check with an LLM.
# This is just a demonstration stub.
def check_contradiction(summary, license_text):
    # You might prompt an LLM here:
    # "Does the summary conflict with or contradict the license? Answer yes or no."
    # We'll just return "No conflict" for demonstration.
    return "No conflict"

# Example usage: pick the license with the highest similarity, do a contradiction check
def best_license(row):
    # find which license has the highest similarity in that row
    row_sim = row[[f"similarity_{l}" for l in licenses]]
    best_lic = row_sim.idxmax()
    # idxmax might return "similarity_MIT", so let's parse out "MIT"
    return best_lic.replace("similarity_", "")

df_summarized["best_license"] = df_summarized.apply(best_license, axis=1)

# Step 5: Summarize & Visualize
# Example summary stats
print("=== Summary Stats ===")
print("Max similarity overall:", df_summarized["license_max_sim"].max())
print("Min similarity overall:", df_summarized["license_max_sim"].min())
print("Rows with high similarity:", df_summarized["high_sim_flag"].sum())
print("Rows with low similarity:", df_summarized["low_sim_flag"].sum())

# Sort by highest max similarity
df_sorted = df_summarized.sort_values(by="license_max_sim", ascending=False)
print("Top 5 answers with highest license similarity:")
print(df_sorted[["answer_summary", "license_max_sim", "best_license"]].head(5))

# Inspect a handful of highest or lowest similarity answers
# For example, contradiction check:
for i in df_sorted.head(5).index:
    row = df_sorted.loc[i]
    chosen_license = row["best_license"]
    # run a hypothetical contradiction check
    result = check_contradiction(row["answer_summary"], license_map[chosen_license])
    print(f"Row {i}: best license={chosen_license}, contradiction check={result}")

# Step 6: (Optional) Visual
# For instance, a simple histogram of the max similarity
import matplotlib.pyplot as plt

df_summarized["license_max_sim"].hist(bins=20)
plt.title("Distribution of Max License Similarity")
plt.xlabel("Similarity")
plt.ylabel("Count of answers")
plt.show()

# That’s it! You now have a script that:
# 1) calculates similarity to multiple licenses,
# 2) interprets results with threshold flags,
# 3) optionally does an LLM contradiction check,
# 4) shows basic summary stats and a histogram visualization.

"""**ZERO SHOT**"""

!pip install pandas PyPDF2 transformers torch

!pip install torchvision

import pandas as pd
import PyPDF2
from transformers import pipeline
import os

df = pd.read_csv("data-ianal.csv")
print(df.head())

pdf_files = [
    "Apache License, Version 2.0 – Open Source Initiative.pdf",
    "Apache Software License, version 1.1 – Open Source Initiative.pdf",
    "GNU Affero General Public License version 3 – Open Source Initiative.pdf",
    "GNU General Public License version 2 – Open Source Initiative.pdf",
    "GNU General Public License version 3 – Open Source Initiative.pdf",
    "GNU Lesser General Public License version 2.1 – Open Source Initiative.pdf",
    "GNU Lesser General Public License version 3 – Open Source Initiative.pdf",
    "GNU Library General Public License version 2 – Open Source Initiative.pdf",
    "The MIT License – Open Source Initiative.pdf",
    "The Nethack General Public License – Open Source Initiative.pdf"
]

license_texts = {}

for pdf_name in pdf_files:
    pdf_path = f"{pdf_name}"
    if os.path.exists(pdf_path):
        with open(pdf_path, "rb") as f:
            reader = PyPDF2.PdfReader(f)
            pages_text = []
            for page in reader.pages:
                text = page.extract_text()
                if text:
                    pages_text.append(text)
            full_text = "\n".join(pages_text)
            license_texts[pdf_name] = full_text
    else:
        print(f"File not found: {pdf_name}")

print("Extracted text for these PDFs:", license_texts.keys())

classifier = pipeline(
    "zero-shot-classification",
    model="facebook/bart-large-mnli"
)

candidate_labels = [
    "MIT",
    "GPL",
    "Apache",
    "AGPL",
    "LGPL",
    "Nethack",
    "No license reference"
]

def zero_shot_classify(text, labels):
    if not isinstance(text, str) or not text.strip():
        return ("No license reference", 1.0)  # if empty, default to "No license reference"

    result = classifier(text, labels)
    # 'labels' are sorted from most likely to least
    top_label = result["labels"][0]
    top_score = result["scores"][0]
    return (top_label, top_score)

df[["zero_shot_label", "zero_shot_score"]] = df["answer_body"].apply(
    lambda ans: pd.Series(zero_shot_classify(ans, candidate_labels))
)

print(df[["answer_body", "zero_shot_label", "zero_shot_score"]])

df.to_csv("data-ianal-zero-shot-labeled.csv", index=False)

HIGH_CONF_THRESHOLD = 0.5
df["is_confident"] = df["zero_shot_score"] > HIGH_CONF_THRESHOLD

print(df[["answer_body", "zero_shot_label", "zero_shot_score","is_confident"]])

"""layman terms"""

!pip install --upgrade transformers accelerate einops bitsandbytes

model_name = "tiiuae/falcon-7b-instruct"

import torch
from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline

model_name = "tiiuae/falcon-7b-instruct"

# Load tokenizer & model
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    device_map="auto",  # let accelerate/bitsandbytes handle GPU usage
    trust_remote_code=True  # needed for some Falcon-based models
)

rewrite_pipeline = pipeline(
    "text-generation",
    model=model,
    tokenizer=tokenizer,
    max_length=1024,
    temperature=0.3,
    do_sample=False
)

def chunk_text(text, chunk_size=1000):
    chunks = []
    start = 0
    while start < len(text):
        end = start + chunk_size
        chunks.append(text[start:end])
        start = end
    return chunks

def rewrite_chunk_in_layman(chunk):
    prompt = f"""
Rewrite the following text in plain, conversational English, suitable for a non-technical audience.
Retain all key obligations, disclaimers, and important details:

{chunk}
"""
    output = rewrite_pipeline(prompt, max_new_tokens=400, num_return_sequences=1)
    # The pipeline returns a list of dicts with 'generated_text'
    rewritten = output[0]["generated_text"]
    # We might want to remove the original prompt from the output if the model echoes it
    # Some instruct models do "prompt + answer." We'll do a simple approach:
    # If the model includes the prompt, we can split or parse it out:
    if prompt in rewritten:
        rewritten = rewritten.split(prompt)[-1].strip()
    return rewritten.strip()

def rewrite_license_in_layman(full_text, chunk_size=1000):
    chunks = chunk_text(full_text, chunk_size)
    layman_parts = []
    for c in chunks:
        layman_text = rewrite_chunk_in_layman(c)
        layman_parts.append(layman_text)
    # Combine all
    combined = "\n\n".join(layman_parts)
    return combined

pdf_mapping = {
    "Apache License, Version 2.0 – Open Source Initiative.pdf": "Apache-2.0",
    "Apache Software License, version 1.1 – Open Source Initiative.pdf": "Apache-1.1",
    "GNU Affero General Public License version 3 – Open Source Initiative.pdf": "AGPL-3.0",
    "GNU General Public License version 2 – Open Source Initiative.pdf": "GPL-2.0",
    "GNU General Public License version 3 – Open Source Initiative.pdf": "GPL-3.0",
    "GNU Lesser General Public License version 2.1 – Open Source Initiative.pdf": "LGPL-2.1",
    "GNU Lesser General Public License version 3 – Open Source Initiative.pdf": "LGPL-3.0",
    "GNU Library General Public License version 2 – Open Source Initiative.pdf": "LGPL-2.0",
    "The MIT License – Open Source Initiative.pdf": "MIT",
    "The Nethack General Public License – Open Source Initiative.pdf": "Nethack"
}

!pip install PyPDF2

import os
import PyPDF2

# Step A: Extract PDF text
def extract_pdf_text(pdf_path):
    pages_text = []
    with open(pdf_path, "rb") as f:
        reader = PyPDF2.PdfReader(f)
        for page in reader.pages:
            txt = page.extract_text()
            if txt:
                pages_text.append(txt)
    return "\n".join(pages_text)

pdf_mapping = {
    "Apache License, Version 2.0 – Open Source Initiative.pdf": "Apache-2.0",
    "Apache Software License, version 1.1 – Open Source Initiative.pdf": "Apache-1.1",
    "GNU Affero General Public License version 3 – Open Source Initiative.pdf": "AGPL-3.0",
    "GNU General Public License version 2 – Open Source Initiative.pdf": "GPL-2.0",
    "GNU General Public License version 3 – Open Source Initiative.pdf": "GPL-3.0",
    "GNU Lesser General Public License version 2.1 – Open Source Initiative.pdf": "LGPL-2.1",
    "GNU Lesser General Public License version 3 – Open Source Initiative.pdf": "LGPL-3.0",
    "GNU Library General Public License version 2 – Open Source Initiative.pdf": "LGPL-2.0",
    "The MIT License – Open Source Initiative.pdf": "MIT",
    "The Nethack General Public License – Open Source Initiative.pdf": "Nethack"
}

license_texts = {}
for lic_key, pdf_file in pdf_mapping.items():
    path = f"{pdf_file}"
    if os.path.exists(path):
        license_texts[lic_key] = extract_pdf_text(path)
    else:
        license_texts[lic_key] = ""

# Step B: Load instruct model
model_name = "tiiuae/falcon-7b-instruct"
tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    device_map="auto",
    trust_remote_code=True
)
rewrite_pipeline = pipeline(
    "text-generation",
    model=model,
    tokenizer=tokenizer,
    max_length=1024,
    temperature=0.3,
    do_sample=False
)

# Step C: Functions
def chunk_text(text, chunk_size=1000):
    return [text[i:i+chunk_size] for i in range(0, len(text), chunk_size)]

def rewrite_chunk_in_layman(chunk):
    prompt = f"""
Rewrite the following text in plain, conversational English, suitable for a non-technical audience.
Retain all key obligations, disclaimers, and important details:

{chunk}
"""
    output = rewrite_pipeline(prompt, max_new_tokens=400)
    rewritten = output[0]["generated_text"]
    # remove repeated prompt if it appears
    if prompt in rewritten:
        rewritten = rewritten.split(prompt)[-1].strip()
    return rewritten.strip()

def rewrite_license_in_layman(full_text):
    if not full_text.strip():
        return ""
    chunks = chunk_text(full_text, 1000)
    results = []
    for c in chunks:
        out = rewrite_chunk_in_layman(c)
        results.append(out)
    # Join
    return "\n\n".join(results)

# Step D: Generate layman version for each license
layman_versions = {}
for lic_key, raw_text in license_texts.items():
    layman_versions[lic_key] = rewrite_license_in_layman(raw_text)

# Step E: Inspect or Save
for k, v in layman_versions.items():
    print("="*40)
    print(f"License: {k}")
    print("Layman version:\n", v)
    print("="*40)

for lic_file, txt in license_texts.items():
    print("License file:", lic_file)
    print("Extracted text snippet:", txt[:500])
    print("="*40)

!pip install pdfplumber

import pdfplumber

with pdfplumber.open("Apache License, Version 2.0 – Open Source Initiative.pdf") as pdf:
    all_text = []
    for page in pdf.pages:
        all_text.append(page.extract_text())
    text = "\n".join(all_text)
    print(text)

pdf_mapping = {
    "Apache License, Version 2.0 – Open Source Initiative.pdf": "Apache-2.0",
    "Apache Software License, version 1.1 – Open Source Initiative.pdf": "Apache-1.1",
    "GNU Affero General Public License version 3 – Open Source Initiative.pdf": "AGPL-3.0",
    "GNU General Public License version 2 – Open Source Initiative.pdf": "GPL-2.0",
    "GNU General Public License version 3 – Open Source Initiative.pdf": "GPL-3.0",
    "GNU Lesser General Public License version 2.1 – Open Source Initiative.pdf": "LGPL-2.1",
    "GNU Lesser General Public License version 3 – Open Source Initiative.pdf": "LGPL-3.0",
    "GNU Library General Public License version 2 – Open Source Initiative.pdf": "LGPL-2.0",
    "The MIT License – Open Source Initiative.pdf": "MIT",
    "The Nethack General Public License – Open Source Initiative.pdf": "Nethack"
}

import pdfplumber
import os

def extract_pdf_text_plumber(pdf_path):
    """
    Extracts text from all pages of a PDF using pdfplumber.
    Returns a single string containing the concatenated text.
    """
    text_list = []
    with pdfplumber.open(pdf_path) as pdf:
        for page in pdf.pages:
            txt = page.extract_text()
            if txt:
                text_list.append(txt)
    return "\n".join(text_list)

license_texts = {}

# 4. Loop through your mapping, read each PDF, store in license_texts
for pdf_file, short_key in pdf_mapping.items():
    pdf_path = f"{pdf_file}"  # Adjust if your path is different
    if os.path.exists(pdf_path):
        full_text = extract_pdf_text_plumber(pdf_path)
        license_texts[short_key] = full_text
        print(f"Extracted {len(full_text)} characters from {pdf_file} -> {short_key}")
    else:
        license_texts[short_key] = ""
        print(f"File not found: {pdf_file}")



for lic_key, text in license_texts.items():
    snippet = text[:300]  # first 300 characters
    print(f"== {lic_key} ==\nExtracted snippet:\n{snippet}\n")

from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline

model_name = "tiiuae/falcon-7b-instruct"  # or "meta-llama/Llama-2-7b-chat-hf" if you have the license
tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    device_map="auto",
    trust_remote_code=True
)
rewrite_pipeline = pipeline(
    "text-generation",
    model=model,
    tokenizer=tokenizer,
    max_length=1024,
    temperature=0.3,
    do_sample=False
)

def rewrite_chunk_in_layman(chunk):
    prompt = f"""
Rewrite the following text in plain, conversational English, suitable for a non-technical audience.
Retain all key obligations, disclaimers, and important details:

{chunk}
"""
    out = rewrite_pipeline(prompt, max_new_tokens=400)
    rewritten = out[0]["generated_text"]
    # remove prompt if it’s echoed
    if prompt in rewritten:
        rewritten = rewritten.split(prompt)[-1].strip()
    return rewritten.strip()

def rewrite_license_in_layman(full_text):
    if not full_text.strip():
        return ""
    chunks = chunk_text(full_text, 1000)
    partials = []
    for c in chunks:
        partials.append(rewrite_chunk_in_layman(c))
    return "\n\n".join(partials)

layman_versions = {}
for short_key, text in license_texts.items():
    layman_versions[short_key] = rewrite_license_in_layman(text)

"""
FULL CODE: Testing a Layman Rewrite Pipeline on a Smaller Subset
===============================================================

Requirements:
  pip install pdfplumber transformers torch accelerate

Note:
  - If your PDFs are truly image-based or have no selectable text,
    pdfplumber won't return anything. You'd need OCR instead.
  - If you want a more "conversational" style, you can replace
    "google/flan-t5-large" with a bigger instruct model
    like "tiiuae/falcon-7b-instruct", but that may require GPU
    and more memory.
"""

import os
import pdfplumber
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline
import torch

##############################################################################
# STEP 1: Define PDF-to-ShortKey Mapping + Optional PDF Extraction
##############################################################################
pdf_mapping = {
    "Apache License, Version 2.0 – Open Source Initiative.pdf": "Apache-2.0",
    "Apache Software License, version 1.1 – Open Source Initiative.pdf": "Apache-1.1",
    "GNU Affero General Public License version 3 – Open Source Initiative.pdf": "AGPL-3.0",
    "GNU General Public License version 2 – Open Source Initiative.pdf": "GPL-2.0",
    "GNU General Public License version 3 – Open Source Initiative.pdf": "GPL-3.0",
    "GNU Lesser General Public License version 2.1 – Open Source Initiative.pdf": "LGPL-2.1",
    "GNU Lesser General Public License version 3 – Open Source Initiative.pdf": "LGPL-3.0",
    "GNU Library General Public License version 2 – Open Source Initiative.pdf": "LGPL-2.0",
    "The MIT License – Open Source Initiative.pdf": "MIT",
    "The Nethack General Public License – Open Source Initiative.pdf": "Nethack"
}

def extract_pdf_text_plumber(pdf_path):
    """
    Extracts text from all pages of a PDF using pdfplumber.
    Returns a single string containing the concatenated text.
    """
    text_list = []
    with pdfplumber.open(pdf_path) as pdf:
        for page in pdf.pages:
            txt = page.extract_text()
            if txt:
                text_list.append(txt)
    return "\n".join(text_list)


# We'll store the text in a dict: {short_key: license_text}
license_texts = {}

##############################################################################
# STEP 2: Read PDFs (Optional) or Load from Pre-Extracted Strings
##############################################################################
# If you have local PDFs and want to parse them, do:
license_dir = ""  # Adjust path as needed

for pdf_file, short_key in pdf_mapping.items():
    pdf_path = os.path.join(license_dir, pdf_file)
    if os.path.exists(pdf_path):
        text = extract_pdf_text_plumber(pdf_path)
        license_texts[short_key] = text
        print(f"Extracted {len(text)} chars from '{pdf_file}' -> {short_key}")
    else:
        # If not found or you prefer to skip
        license_texts[short_key] = ""
        print(f"File not found or skipping: {pdf_file}")

##############################################################################
# STEP 3: Chunking Function
##############################################################################
def chunk_text(text, chunk_size=1000):
    """
    Breaks the text into segments of chunk_size characters each
    to avoid model input length issues.
    """
    chunks = []
    start = 0
    while start < len(text):
        end = start + chunk_size
        chunks.append(text[start:end])
        start = end
    return chunks

##############################################################################
# STEP 4: Initialize a Summarization or Instruct Pipeline
##############################################################################
# Example: Using FLAN-T5 for more flexible rewriting (somewhere between summarization and instruct).
# You can use a smaller model if needed: "google/flan-t5-base" or "google/flan-t5-large".
# Or an instruct model like "tiiuae/falcon-7b-instruct" if you have GPU memory available.

model_name = "google/flan-t5-large"  # Smaller than falcon, more CPU-friendly

tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForSeq2SeqLM.from_pretrained(model_name, device_map="auto")

rewrite_pipeline = pipeline(
    "text2text-generation",  # for Seq2Seq models
    model=model,
    tokenizer=tokenizer,
    max_length=512,
    # We can keep do_sample=False for deterministic style
    do_sample=False
)

##############################################################################
# STEP 5: Layman Rewrite Functions
##############################################################################
def rewrite_chunk_in_layman(chunk):
    """
    Creates a prompt asking the model to rewrite the chunk in plain,
    conversational English. Then returns the model's generated text.
    """
    if not chunk.strip():
        return ""
    prompt = f"""
Rewrite the following text in plain, conversational English, suitable for a non-technical audience,
while retaining any important obligations, disclaimers, and details:

{chunk}
"""
    outputs = rewrite_pipeline(prompt, max_new_tokens=200)
    # For flan-t5, outputs is a list of dicts with 'generated_text'
    rewritten = outputs[0]["generated_text"]
    return rewritten.strip()

def rewrite_license_in_layman(full_text):
    """
    Splits the license text into chunks and rewrites each chunk.
    """
    if not full_text.strip():
        return ""
    # We'll chunk at size=1000, can reduce or increase if needed
    chunks = chunk_text(full_text, 1000)
    partials = []
    for i, c in enumerate(chunks):
        print(f"  Rewriting chunk {i+1}/{len(chunks)} (length={len(c)}) ...")
        partials.append(rewrite_chunk_in_layman(c))
    return "\n\n".join(partials)

##############################################################################
# STEP 6: Test on a Smaller Subset First
##############################################################################
print("\n======== Testing on a Subset of Licenses ========")

# Let's just take 1 or 2 licenses to confirm it works quickly
test_keys = ["Apache-2.0", "MIT"]  # or any subset
for k in test_keys:
    text = license_texts.get(k, "")
    truncated_text = text[:2000]  # Only first 2000 chars to speed test
    print(f"\nRewriting {k} (truncated to 2000 chars, original len={len(text)}) ...")
    layman_result = rewrite_license_in_layman(truncated_text)
    print(f"=== Layman version for {k} ===\n", layman_result[:500], "...\n")

##############################################################################
# STEP 7 (Optional): Full Run on All Licenses
##############################################################################
# Once confirmed, you could do the entire set (which may be long-running).
# Example:
"""
layman_versions = {}
for short_key, full_text in license_texts.items():
    print(f"\nProcessing {short_key} - length={len(full_text)}")
    # Possibly skip if empty
    if not full_text.strip():
        layman_versions[short_key] = ""
        continue
    # Or chunk and rewrite everything
    layman_versions[short_key] = rewrite_license_in_layman(full_text)

# Then save or print
import json
with open("layman_versions.json", "w") as f:
    json.dump(layman_versions, f, indent=2)
"""

print("\n[Done] You can now verify the shortened output from the test subset.")

import os
import pdfplumber
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline

##############################################################################
# STEP 1: PDF-to-ShortKey Mapping
##############################################################################
pdf_mapping = {
    "Apache License, Version 2.0 – Open Source Initiative.pdf": "Apache-2.0",
    "Apache Software License, version 1.1 – Open Source Initiative.pdf": "Apache-1.1",
    "GNU Affero General Public License version 3 – Open Source Initiative.pdf": "AGPL-3.0",
    "GNU General Public License version 2 – Open Source Initiative.pdf": "GPL-2.0",
    "GNU General Public License version 3 – Open Source Initiative.pdf": "GPL-3.0",
    "GNU Lesser General Public License version 2.1 – Open Source Initiative.pdf": "LGPL-2.1",
    "GNU Lesser General Public License version 3 – Open Source Initiative.pdf": "LGPL-3.0",
    "GNU Library General Public License version 2 – Open Source Initiative.pdf": "LGPL-2.0",
    "The MIT License – Open Source Initiative.pdf": "MIT",
    "The Nethack General Public License – Open Source Initiative.pdf": "Nethack"
}

##############################################################################
# STEP 2: PDF Extraction with pdfplumber
##############################################################################
def extract_pdf_text_plumber(pdf_path):
    """
    Extracts text from all pages of a PDF using pdfplumber.
    Returns a single string containing the concatenated text.
    """
    text_list = []
    with pdfplumber.open(pdf_path) as pdf:
        for page in pdf.pages:
            txt = page.extract_text()
            if txt:
                text_list.append(txt)
    return "\n".join(text_list)

license_texts = {}

license_dir = ""  # Adjust your directory path if needed

for pdf_file, short_key in pdf_mapping.items():
    pdf_path = os.path.join(license_dir, pdf_file)
    if os.path.exists(pdf_path):
        text = extract_pdf_text_plumber(pdf_path)
        license_texts[short_key] = text
        print(f"Extracted {len(text)} chars from '{pdf_file}' -> {short_key}")
    else:
        # If file not found or empty, store empty string
        license_texts[short_key] = ""
        print(f"File not found or skipping: {pdf_file}")

##############################################################################
# STEP 3: Chunking Function
##############################################################################
def chunk_text(text, chunk_size=1000):
    """
    Breaks the text into segments of chunk_size characters each.
    """
    chunks = []
    start = 0
    while start < len(text):
        end = start + chunk_size
        chunks.append(text[start:end])
        start = end
    return chunks

##############################################################################
# STEP 4: Initialize a Summarization / Rewriting Model
##############################################################################
model_name = "google/flan-t5-large"  # For example, smaller than big instruct models
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForSeq2SeqLM.from_pretrained(model_name, device_map="auto")

rewrite_pipeline = pipeline(
    "text2text-generation",
    model=model,
    tokenizer=tokenizer,
    max_length=512,
    do_sample=False
)

##############################################################################
# STEP 5: Functions to Rewrite Text in Layman
##############################################################################
def rewrite_chunk_in_layman(chunk):
    if not chunk.strip():
        return ""
    prompt = f"""
Rewrite the following text in plain, conversational English, suitable for a non-technical audience,
while retaining any important obligations, disclaimers, and details:

{chunk}
"""
    outputs = rewrite_pipeline(prompt, max_new_tokens=200)
    rewritten = outputs[0]["generated_text"]
    return rewritten.strip()

def rewrite_license_in_layman(full_text):
    if not full_text.strip():
        return ""
    chunks = chunk_text(full_text, 1000)
    partials = []
    for i, c in enumerate(chunks):
        print(f"  Rewriting chunk {i+1}/{len(chunks)} (length={len(c)}) ...")
        partials.append(rewrite_chunk_in_layman(c))
    return "\n\n".join(partials)

##############################################################################
# STEP 6: Rewrite All Licenses & Save to Separate Files
##############################################################################
output_folder = "./layman_outputs"
os.makedirs(output_folder, exist_ok=True)

for short_key, full_text in license_texts.items():
    print(f"\nProcessing {short_key} (length={len(full_text)} chars) ...")
    if not full_text.strip():
        print("No text found, skipping.")
        continue
    # Rewrite entire text
    layman_text = rewrite_license_in_layman(full_text)

    # Save to a .txt file
    output_file = os.path.join(output_folder, f"layman_{short_key}.txt")
    with open(output_file, "w", encoding="utf-8") as f:
        f.write(layman_text)
    print(f"Saved layman version to: {output_file}")

print("\n[Done] All PDFs processed. Layman text files are in", output_folder)

# 1. Zip the folder (and its contents)
!zip -r layman_outputs.zip /content/layman_outputs

# 2. Download the zipped folder
from google.colab import files
files.download('layman_outputs.zip')

# 1. Zip the folder (and its contents)
!zip -r twostep_layman.zip /content/twostep_layman

# 2. Download the zipped folder
from google.colab import files
files.download('twostep_layman.zip')

import os
import pdfplumber
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline

##############################################################################
# 0. PDF Mapping
##############################################################################
pdf_mapping = {
    "Apache License, Version 2.0 – Open Source Initiative.pdf": "Apache-2.0",
    "Apache Software License, version 1.1 – Open Source Initiative.pdf": "Apache-1.1",
    "GNU Affero General Public License version 3 – Open Source Initiative.pdf": "AGPL-3.0",
    "GNU General Public License version 2 – Open Source Initiative.pdf": "GPL-2.0",
    "GNU General Public License version 3 – Open Source Initiative.pdf": "GPL-3.0",
    "GNU Lesser General Public License version 2.1 – Open Source Initiative.pdf": "LGPL-2.1",
    "GNU Lesser General Public License version 3 – Open Source Initiative.pdf": "LGPL-3.0",
    "GNU Library General Public License version 2 – Open Source Initiative.pdf": "LGPL-2.0",
    "The MIT License – Open Source Initiative.pdf": "MIT",
    "The Nethack General Public License – Open Source Initiative.pdf": "Nethack"
}

license_dir = ""  # Adjust if needed

##############################################################################
# 1. PDF Extraction Function
##############################################################################
def extract_pdf_text_plumber(pdf_path):
    """
    Extract text from all pages of a PDF using pdfplumber.
    Returns a single string containing the concatenated text.
    """
    all_text = []
    with pdfplumber.open(pdf_path) as pdf:
        for page in pdf.pages:
            txt = page.extract_text()
            if txt:
                all_text.append(txt)
    return "\n".join(all_text)

##############################################################################
# 2. Chunking Function
##############################################################################
def chunk_text(text: str, chunk_size=1000):
    """
    Breaks the input text into segments of `chunk_size` characters each.
    """
    chunks = []
    start = 0
    while start < len(text):
        end = start + chunk_size
        chunks.append(text[start:end])
        start = end
    return chunks

##############################################################################
# 3. Two-Pass Layman Rewrite
##############################################################################
def two_pass_layman(chunk: str, pipeline_func) -> str:
    """
    1) Summarize the chunk in ~5-7 sentences.
    2) Rewrite that summary in a friendly, non-technical style,
       avoiding direct copying from the original text.
    """
    if not chunk.strip():
        return ""

    # Step 1: Summarize
    prompt_summary = f"Summarize this text in 5-7 sentences max:\n\n{chunk}"
    summary_out = pipeline_func(prompt_summary, max_new_tokens=200)[0]["generated_text"]

    # Step 2: Layman rewrite
    prompt_layman = f"""
Rewrite this summary in a friendly, non-technical style.
Avoid copying phrases from the original.
Emphasize main obligations and disclaimers:

{summary_out}
"""
    layman_out = pipeline_func(prompt_layman, max_new_tokens=300)[0]["generated_text"]

    return layman_out.strip()

def rewrite_license_in_layman(full_text: str, pipeline_func) -> str:
    """
    Splits text into chunks, applies two_pass_layman to each,
    and concatenates results.
    """
    if not full_text.strip():
        return ""

    chunks = chunk_text(full_text, 1000)
    results = []
    for i, ch in enumerate(chunks):
        print(f"  Rewriting chunk {i+1}/{len(chunks)} (length={len(ch)})...")
        results.append(two_pass_layman(ch, pipeline_func))

    return "\n\n".join(results)

##############################################################################
# 4. Initialize the Model & Pipeline
##############################################################################
model_name = "google/flan-t5-large"  # Or any other model, e.g. Falcon Instruct
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForSeq2SeqLM.from_pretrained(model_name, device_map="auto")

rewrite_pipeline = pipeline(
    "text2text-generation",
    model=model,
    tokenizer=tokenizer,
    max_length=512,
    do_sample=True,     # sampling to encourage paraphrasing
    temperature=0.7,
    top_p=0.9,
    top_k=50
)

##############################################################################
# 5. Main Execution: Extract, Rewrite, Save
##############################################################################
def main():
    # 5A. Create a dictionary of extracted PDF texts
    license_texts = {}
    for pdf_file, short_key in pdf_mapping.items():
        pdf_path = os.path.join(license_dir, pdf_file)
        if os.path.exists(pdf_path):
            text = extract_pdf_text_plumber(pdf_path)
            license_texts[short_key] = text
            print(f"Extracted {len(text)} chars from '{pdf_file}' -> {short_key}")
        else:
            license_texts[short_key] = ""
            print(f"File not found or skipping: {pdf_file}")

    # 5B. Create output folder
    output_folder = "twostep_layman"
    os.makedirs(output_folder, exist_ok=True)

    # 5C. Rewrite each license
    for short_key, text in license_texts.items():
        print(f"\nProcessing {short_key} (length={len(text)})")
        if not text.strip():
            print("  No text found, skipping rewrite.")
            continue

        # Two-step rewriting
        layman_version = rewrite_license_in_layman(text, rewrite_pipeline)

        # Save to file
        out_filename = f"twostep_{short_key}.txt"
        out_path = os.path.join(output_folder, out_filename)
        with open(out_path, "w", encoding="utf-8") as f:
            f.write(layman_version)

        print(f"  Saved layman version to: {out_path}")

    print("\n[Done] All PDFs processed. Rewritten text in folder:", output_folder)

if __name__ == "__main__":
    main()

import os
import pdfplumber
import torch
from transformers import (
    AutoTokenizer,
    AutoModelForCausalLM,
    pipeline
)

##############################################################################
# 0. PDF Mapping
##############################################################################
pdf_mapping = {
    "Apache License, Version 2.0 – Open Source Initiative.pdf": "Apache-2.0",
    "Apache Software License, version 1.1 – Open Source Initiative.pdf": "Apache-1.1",
    "GNU Affero General Public License version 3 – Open Source Initiative.pdf": "AGPL-3.0",
    "GNU General Public License version 2 – Open Source Initiative.pdf": "GPL-2.0",
    "GNU General Public License version 3 – Open Source Initiative.pdf": "GPL-3.0",
    "GNU Lesser General Public License version 2.1 – Open Source Initiative.pdf": "LGPL-2.1",
    "GNU Lesser General Public License version 3 – Open Source Initiative.pdf": "LGPL-3.0",
    "GNU Library General Public License version 2 – Open Source Initiative.pdf": "LGPL-2.0",
    "The MIT License – Open Source Initiative.pdf": "MIT",
    "The Nethack General Public License – Open Source Initiative.pdf": "Nethack"
}

license_dir = ""  # Adjust if needed

##############################################################################
# 1. PDF Extraction
##############################################################################
def extract_pdf_text_plumber(pdf_path):
    """
    Extract text from all pages of a PDF using pdfplumber.
    Returns a single string containing the concatenated text.
    """
    all_text = []
    with pdfplumber.open(pdf_path) as pdf:
        for page in pdf.pages:
            txt = page.extract_text()
            if txt:
                all_text.append(txt)
    return "\n".join(all_text)

##############################################################################
# 2. Chunking Function
##############################################################################
def chunk_text(text: str, chunk_size=1000):
    """
    Breaks the input text into segments of `chunk_size` characters each.
    """
    chunks = []
    start = 0
    while start < len(text):
        end = start + chunk_size
        chunks.append(text[start:end])
        start = end
    return chunks

##############################################################################
# 3. Two-Pass Layman Rewrite
##############################################################################
def two_pass_layman(chunk: str, rewrite_fn) -> str:
    """
    1) Summarize the chunk in ~5-7 sentences.
    2) Rewrite that summary in a friendly, non-technical style,
       avoiding direct copying from the original text.

    :param chunk: The portion of text to transform.
    :param rewrite_fn: a function/pipeline that does the text generation calls.
    """
    if not chunk.strip():
        return ""

    # Step 1: Summarize
    prompt_summary = (
        "Summarize this text in 5-7 sentences max.\n\n"
        + chunk
    )
    summary_out = rewrite_fn(prompt_summary, max_new_tokens=200)

    # The model returns a list[dict], we take the first "generated_text"
    summary_text = summary_out[0]["generated_text"]

    # Step 2: Layman rewrite
    prompt_layman = f"""
Rewrite this summary in a friendly, non-technical style.
Avoid copying phrases from the original.
Emphasize main obligations and disclaimers:

{summary_text}
"""
    layman_out = rewrite_fn(prompt_layman, max_new_tokens=300)
    final_text = layman_out[0]["generated_text"].strip()

    return final_text

##############################################################################
# 4. Full Rewrite on License
##############################################################################
def rewrite_license_in_layman(full_text: str, rewrite_fn) -> str:
    """
    Splits text into chunks, applies two_pass_layman to each,
    and concatenates results.
    """
    if not full_text.strip():
        return ""

    # Split into 1k-char chunks
    chunks = chunk_text(full_text, 1000)
    results = []
    for i, ch in enumerate(chunks):
        print(f"  Rewriting chunk {i+1}/{len(chunks)} (length={len(ch)} chars)...")
        out = two_pass_layman(ch, rewrite_fn)
        results.append(out)

    return "\n\n".join(results)

##############################################################################
# 5. Initialize a Chatty / Instruct Model (Falcon 7B Instruct Example)
##############################################################################
def init_falcon_instruct_pipeline():
    """
    Creates a text-generation pipeline with a chatty/instruct model
    like 'tiiuae/falcon-7b-instruct'.
    Make sure you have enough GPU RAM or a suitable environment.
    """
    model_name = "tiiuae/falcon-7b-instruct"  # or "meta-llama/Llama-2-7b-chat-hf" w/ license
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
    model = AutoModelForCausalLM.from_pretrained(
        model_name,
        device_map="auto",
        trust_remote_code=True
    )
    # We'll enable sampling + moderate temperature for paraphrasing
    instruct_pipeline = pipeline(
        "text-generation",
        model=model,
        tokenizer=tokenizer,
        max_length=1024,
        do_sample=True,
        temperature=0.7,
        top_p=0.9,
        top_k=50
    )
    return instruct_pipeline

##############################################################################
# 6. Main Execution
##############################################################################
def main():
    # Step A: Extract text for each license
    license_texts = {}
    for pdf_file, short_key in pdf_mapping.items():
        pdf_path = os.path.join(license_dir, pdf_file)
        if os.path.exists(pdf_path):
            text = extract_pdf_text_plumber(pdf_path)
            license_texts[short_key] = text
            print(f"Extracted {len(text)} chars from '{pdf_file}' -> {short_key}")
        else:
            license_texts[short_key] = ""
            print(f"File not found or skipping: {pdf_file}")

    # Step B: Initialize the Falcon instruct pipeline
    rewrite_pipeline = init_falcon_instruct_pipeline()

    # Step C: Make output folder
    output_folder = "new_layman"
    os.makedirs(output_folder, exist_ok=True)

    # Step D: Rewrite each license using two-step approach
    for short_key, text in license_texts.items():
        print(f"\nProcessing {short_key} (length={len(text)})...")
        if not text.strip():
            print("  No text found, skipping rewrite.")
            continue

        layman_text = rewrite_license_in_layman(text, rewrite_pipeline)

        # Save
        out_file = os.path.join(output_folder, f"twostep_{short_key}.txt")
        with open(out_file, "w", encoding="utf-8") as f:
            f.write(layman_text)

        print(f"  Saved layman version to: {out_file}")

    print("\nDone! Rewritten texts are in folder:", output_folder)

if __name__ == "__main__":
    main()

import os
import pdfplumber
from transformers import (
    AutoTokenizer,
    AutoModelForCausalLM,
    pipeline
)
from transformers import pipeline as sum_pipeline  # for summarization

def extract_pdf_text(pdf_path):
    texts = []
    with pdfplumber.open(pdf_path) as pdf:
        for page in pdf.pages:
            txt = page.extract_text()
            if txt:
                texts.append(txt.strip())
    return "\n".join(texts)

def chunk_text(text, chunk_size=500):
    out = []
    start = 0
    while start < len(text):
        out.append(text[start:start+chunk_size])
        start += chunk_size
    return out

def load_instruct_pipeline():
    model_name = "tiiuae/falcon-7b-instruct"  # or "meta-llama/Llama-2-7b-chat-hf"
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
    model = AutoModelForCausalLM.from_pretrained(
        model_name,
        device_map="auto",
        trust_remote_code=True
    )
    instruct_pl = pipeline(
        "text-generation",
        model=model,
        tokenizer=tokenizer,
        max_length=512,
        do_sample=True,
        temperature=1.0,  # push higher
        top_p=0.95,
        top_k=0
    )
    return instruct_pl

def two_pass_layman(chunk, instruct_pipe, summarizer_pipe):
    # 1) Summarize with a standard summarizer
    summary_res = summarizer_pipe(chunk, max_length=100, min_length=30, do_sample=False)
    short_summary = summary_res[0]["summary_text"]

    # 2) Use instruct model for a "layman rewrite"
    # Provide strong instructions
    prompt = f"""Rewrite the following summary in simpler, casual English.
Avoid copying any exact phrasing from the summary.
Focus on the key obligations, disclaimers, or usage rules:
SUMMARY:
"{short_summary}"
"""
    out = instruct_pipe(prompt, max_new_tokens=200)
    text_out = out[0]["generated_text"]

    # Optionally strip the prompt if it appears
    if prompt in text_out:
        text_out = text_out.split(prompt)[-1].strip()

    return text_out.strip()

def process_license(full_text, instruct_pipe, summarizer_pipe):
    if not full_text.strip():
        return ""
    chunks = chunk_text(full_text, 500)
    all_rewrites = []
    for i, ch in enumerate(chunks):
        res = two_pass_layman(ch, instruct_pipe, summarizer_pipe)
        all_rewrites.append(res)
    return "\n\n".join(all_rewrites)

def main():
    pdf_mapping = {
    "Apache License, Version 2.0 – Open Source Initiative.pdf": "Apache-2.0",
    "Apache Software License, version 1.1 – Open Source Initiative.pdf": "Apache-1.1",
    "GNU Affero General Public License version 3 – Open Source Initiative.pdf": "AGPL-3.0",
    "GNU General Public License version 2 – Open Source Initiative.pdf": "GPL-2.0",
    "GNU General Public License version 3 – Open Source Initiative.pdf": "GPL-3.0",
    "GNU Lesser General Public License version 2.1 – Open Source Initiative.pdf": "LGPL-2.1",
    "GNU Lesser General Public License version 3 – Open Source Initiative.pdf": "LGPL-3.0",
    "GNU Library General Public License version 2 – Open Source Initiative.pdf": "LGPL-2.0",
    "The MIT License – Open Source Initiative.pdf": "MIT",
    "The Nethack General Public License – Open Source Initiative.pdf": "Nethack"
}
    license_dir = ""
    summarizer_pipe = sum_pipeline("summarization", model="facebook/bart-large-cnn")
    instruct_pipe = load_instruct_pipeline()

    out_dir = "twostep_layman_enhanced"
    os.makedirs(out_dir, exist_ok=True)

    for pdf_file, short_key in pdf_mapping.items():
        pdf_path = os.path.join(license_dir, pdf_file)
        if not os.path.exists(pdf_path):
            continue
        text = extract_pdf_text(pdf_path)
        print(f"Processing {short_key}... len={len(text)}")

        layman_text = process_license(text, instruct_pipe, summarizer_pipe)
        out_file = os.path.join(out_dir, f"twostep_{short_key}.txt")
        with open(out_file, "w", encoding="utf-8") as f:
            f.write(layman_text)
        print(f"Saved => {out_file}")

if __name__ == "__main__":
    main()